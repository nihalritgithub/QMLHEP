{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11001930,"sourceType":"datasetVersion","datasetId":6848828}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task II: Classical Graph Neural Network (GNN)\n\nFor Task II, you will use ParticleNet’s data for Quark/Gluon jet classification available here with its corresponding description.\n\n* Choose 2 Graph-based architectures of your choice to classify jets as being quarks or gluons. Provide a description on what considerations you have taken to project this point-cloud dataset to a set of interconnected nodes and edges.\n* Discuss the resulting performance of the 2 chosen architectures.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html\n!pip install torch-sparse -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html\n!pip install torch-cluster -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html\n!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html\n!pip install torch-geometric\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import EdgeConv, global_mean_pool, GATConv\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_cluster import knn_graph\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\ndef load_and_preprocess(file_path, num_particles=100, k=16):\n    data = np.load(file_path)\n    features = data['X']\n    labels = data['y']\n\n    processed_data = []\n    for i in range(features.shape[0]):\n        jet_features = features[i]\n        if jet_features.shape[0] > num_particles:\n            jet_features = jet_features[:num_particles]\n        elif jet_features.shape[0] < num_particles:\n            padding = np.zeros((num_particles - jet_features.shape[0], 4))\n            jet_features = np.concatenate([jet_features, padding], axis=0)\n\n        jet_features[:, 0] = (jet_features[:, 0] - np.mean(jet_features[:, 0])) / np.std(jet_features[:, 0])\n        jet_features[:, 1] = (jet_features[:, 1] - np.mean(jet_features[:, 1])) / np.std(jet_features[:, 1])\n        jet_features[:, 2] = (jet_features[:, 2] - np.mean(jet_features[:, 2])) / np.std(jet_features[:, 2])\n        jet_features[:, 3] = (jet_features[:, 3] - np.mean(jet_features[:, 3])) / np.std(jet_features[:, 3])\n\n        x = torch.tensor(jet_features, dtype=torch.float)\n        y = torch.tensor(labels[i], dtype=torch.long)\n\n        edge_index_knn = knn_graph(x[:, 1:3], k=k, batch=None)\n        num_nodes = x.size(0)\n        edge_index_fc = torch.combinations(torch.arange(num_nodes), r=2).t()\n        edge_index_fc = torch.cat([edge_index_fc, edge_index_fc.flip(0)], dim=1)\n\n        data_knn = Data(x=x, edge_index=edge_index_knn, y=y)\n        data_fc = Data(x=x, edge_index=edge_index_fc, y=y)\n        processed_data.append((data_knn, data_fc))\n\n    return processed_data\n\n\nclass EdgeConvNet(torch.nn.Module):\n    def __init__(self, num_features, num_classes, k=16):\n        super(EdgeConvNet, self).__init__()\n        self.k = k\n        self.conv1 = EdgeConv(nn.Sequential(nn.Linear(2 * num_features, 64), nn.ReLU(), nn.Linear(64, 64)), aggr='max')\n        self.conv2 = EdgeConv(nn.Sequential(nn.Linear(\n            2 * 64, 128), nn.ReLU(), nn.Linear(128, 128)), aggr='max')\n        self.lin1 = nn.Linear(128, 256)\n        self.lin2 = nn.Linear(256, num_classes)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        x = self.conv2(x, edge_index)\n        x = global_mean_pool(x, batch)\n        x = F.relu(self.lin1(x))\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=1)\n\nclass GATNet(torch.nn.Module):\n    def __init__(self, num_features, num_classes, hidden_dim=64, heads=4):\n        super(GATNet, self).__init__()\n        self.conv1 = GATConv(num_features, hidden_dim, heads=heads)\n        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads)\n        self.lin1 = nn.Linear(hidden_dim * heads, 256)\n        self.lin2 = nn.Linear(256, num_classes)\n\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = F.relu(self.conv1(x, edge_index))\n        x = F.relu(self.conv2(x, edge_index))\n        x = global_mean_pool(x, batch)\n        x = F.relu(self.lin1(x))\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=1)\n\ndef train(model, train_loader, optimizer, device):\n    model.train()\n    total_loss = 0\n    for data_list in train_loader:\n        data = data_list[0] if isinstance(model, EdgeConvNet) else data_list[1]\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\n\n\ndef evaluate(model, loader, device):\n    model.eval()\n    correct = 0\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for data_list in loader:\n            data = data_list[0] if isinstance(model, EdgeConvNet) else data_list[1]\n            data = data.to(device)\n            out = model(data)\n            pred = out.argmax(dim=1)\n            correct += int((pred == data.y).sum())\n            all_preds.extend(torch.exp(out)[:, 1].cpu().numpy())\n            all_labels.extend(data.y.cpu().numpy())\n\n    acc = correct / len(loader.dataset)\n    auc = roc_auc_score(all_labels, all_preds)\n    return acc, auc\n\n\nif __name__ == '__main__':\n    file_path = '/kaggle/input/qg-jets/QG_jets.npz'\n    num_particles = 100\n    k = 16\n    batch_size = 32\n    epochs = 20\n    learning_rate = 0.001\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    processed_data = load_and_preprocess(file_path, num_particles=num_particles, k=k)\n\n    train_data, test_data = train_test_split(processed_data, test_size=0.2, random_state=42)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\n    edgeconv_model = EdgeConvNet(num_features=4, num_classes=2, k=k).to(device)\n    edgeconv_optimizer = torch.optim.Adam(edgeconv_model.parameters(), lr=learning_rate)\n\n    print(\"Training EdgeConv Model...\")\n    for epoch in range(1, epochs + 1):\n        loss = train(edgeconv_model, train_loader, edgeconv_optimizer, device)\n        train_acc, train_auc = evaluate(edgeconv_model, train_loader, device)\n        test_acc, test_auc = evaluate(edgeconv_model, test_loader, device)\n        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Train AUC: {train_auc:.4f}, Test Acc: {test_acc:.4f}, Test AUC: {test_auc:.4f}')\n\n    gat_model = GATNet(num_features=4, num_classes=2).to(device)\n    gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=learning_rate)\n\n    print(\"\\nTraining GAT Model...\")\n    for epoch in range(1, epochs + 1):\n        loss = train(gat_model, train_loader, gat_optimizer, device)\n        train_acc, train_auc = evaluate(gat_model, train_loader, device)\n        test_acc, test_auc = evaluate(gat_model, test_loader, device)\n        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Train AUC: {train_auc:.4f}, Test Acc: {test_acc:.4f}, Test AUC: {test_auc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T17:56:35.108195Z","iopub.execute_input":"2025-03-17T17:56:35.108465Z","iopub.status.idle":"2025-03-17T19:22:29.029157Z","shell.execute_reply.started":"2025-03-17T17:56:35.108437Z","shell.execute_reply":"2025-03-17T19:22:29.028056Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2+pt25cu121\nLooking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\nCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (5.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\nRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2024.2.0)\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.18+pt25cu121\nLooking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\nCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_cluster-1.6.3%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.13.1)\nRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-cluster) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-cluster) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-cluster) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-cluster) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-cluster) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-cluster) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy->torch-cluster) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy->torch-cluster) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy->torch-cluster) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy->torch-cluster) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy->torch-cluster) (2024.2.0)\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.3+pt25cu121\nLooking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\nCollecting torch-spline-conv\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (991 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m991.6/991.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-spline-conv\nSuccessfully installed torch-spline-conv-1.2.2+pt25cu121\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.12)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.12.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\nTraining EdgeConv Model...\nEpoch: 01, Loss: 0.4784, Train Acc: 0.7911, Train AUC: 0.8658, Test Acc: 0.7961, Test AUC: 0.8693\nEpoch: 02, Loss: 0.4652, Train Acc: 0.7943, Train AUC: 0.8684, Test Acc: 0.7984, Test AUC: 0.8721\nEpoch: 03, Loss: 0.4608, Train Acc: 0.7942, Train AUC: 0.8699, Test Acc: 0.7964, Test AUC: 0.8729\nEpoch: 04, Loss: 0.4584, Train Acc: 0.7951, Train AUC: 0.8713, Test Acc: 0.8001, Test AUC: 0.8749\nEpoch: 05, Loss: 0.4550, Train Acc: 0.7949, Train AUC: 0.8732, Test Acc: 0.7992, Test AUC: 0.8763\nEpoch: 06, Loss: 0.4532, Train Acc: 0.7927, Train AUC: 0.8730, Test Acc: 0.7944, Test AUC: 0.8761\nEpoch: 07, Loss: 0.4518, Train Acc: 0.7994, Train AUC: 0.8746, Test Acc: 0.8011, Test AUC: 0.8770\nEpoch: 08, Loss: 0.4500, Train Acc: 0.7997, Train AUC: 0.8757, Test Acc: 0.8028, Test AUC: 0.8779\nEpoch: 09, Loss: 0.4478, Train Acc: 0.8001, Train AUC: 0.8766, Test Acc: 0.8016, Test AUC: 0.8782\nEpoch: 10, Loss: 0.4471, Train Acc: 0.8004, Train AUC: 0.8777, Test Acc: 0.8014, Test AUC: 0.8780\nEpoch: 11, Loss: 0.4458, Train Acc: 0.7984, Train AUC: 0.8778, Test Acc: 0.8023, Test AUC: 0.8788\nEpoch: 12, Loss: 0.4446, Train Acc: 0.8007, Train AUC: 0.8792, Test Acc: 0.8010, Test AUC: 0.8793\nEpoch: 13, Loss: 0.4435, Train Acc: 0.8033, Train AUC: 0.8799, Test Acc: 0.8018, Test AUC: 0.8792\nEpoch: 14, Loss: 0.4422, Train Acc: 0.8033, Train AUC: 0.8808, Test Acc: 0.8023, Test AUC: 0.8794\nEpoch: 15, Loss: 0.4413, Train Acc: 0.8037, Train AUC: 0.8805, Test Acc: 0.8037, Test AUC: 0.8792\nEpoch: 16, Loss: 0.4398, Train Acc: 0.8046, Train AUC: 0.8820, Test Acc: 0.8030, Test AUC: 0.8790\nEpoch: 17, Loss: 0.4386, Train Acc: 0.8051, Train AUC: 0.8834, Test Acc: 0.8038, Test AUC: 0.8801\nEpoch: 18, Loss: 0.4377, Train Acc: 0.8068, Train AUC: 0.8846, Test Acc: 0.8052, Test AUC: 0.8802\nEpoch: 19, Loss: 0.4362, Train Acc: 0.8084, Train AUC: 0.8852, Test Acc: 0.8044, Test AUC: 0.8798\nEpoch: 20, Loss: 0.4347, Train Acc: 0.8089, Train AUC: 0.8862, Test Acc: 0.8042, Test AUC: 0.8791\n\nTraining GAT Model...\nEpoch: 01, Loss: 0.5040, Train Acc: 0.7464, Train AUC: 0.8440, Test Acc: 0.7430, Test AUC: 0.8470\nEpoch: 02, Loss: 0.4915, Train Acc: 0.7757, Train AUC: 0.8482, Test Acc: 0.7784, Test AUC: 0.8511\nEpoch: 03, Loss: 0.4884, Train Acc: 0.7767, Train AUC: 0.8498, Test Acc: 0.7806, Test AUC: 0.8534\nEpoch: 04, Loss: 0.4850, Train Acc: 0.7693, Train AUC: 0.8449, Test Acc: 0.7724, Test AUC: 0.8475\nEpoch: 05, Loss: 0.4825, Train Acc: 0.7797, Train AUC: 0.8518, Test Acc: 0.7817, Test AUC: 0.8543\nEpoch: 06, Loss: 0.4808, Train Acc: 0.7780, Train AUC: 0.8525, Test Acc: 0.7796, Test AUC: 0.8555\nEpoch: 07, Loss: 0.4800, Train Acc: 0.7761, Train AUC: 0.8539, Test Acc: 0.7766, Test AUC: 0.8566\nEpoch: 08, Loss: 0.4789, Train Acc: 0.7810, Train AUC: 0.8537, Test Acc: 0.7850, Test AUC: 0.8565\nEpoch: 09, Loss: 0.4787, Train Acc: 0.7799, Train AUC: 0.8532, Test Acc: 0.7800, Test AUC: 0.8552\nEpoch: 10, Loss: 0.4779, Train Acc: 0.7803, Train AUC: 0.8533, Test Acc: 0.7803, Test AUC: 0.8559\nEpoch: 11, Loss: 0.4776, Train Acc: 0.7802, Train AUC: 0.8543, Test Acc: 0.7806, Test AUC: 0.8564\nEpoch: 12, Loss: 0.4771, Train Acc: 0.7796, Train AUC: 0.8535, Test Acc: 0.7814, Test AUC: 0.8562\nEpoch: 13, Loss: 0.4758, Train Acc: 0.7809, Train AUC: 0.8543, Test Acc: 0.7824, Test AUC: 0.8569\nEpoch: 14, Loss: 0.4757, Train Acc: 0.7807, Train AUC: 0.8549, Test Acc: 0.7828, Test AUC: 0.8575\nEpoch: 15, Loss: 0.4755, Train Acc: 0.7818, Train AUC: 0.8556, Test Acc: 0.7819, Test AUC: 0.8576\nEpoch: 16, Loss: 0.4756, Train Acc: 0.7768, Train AUC: 0.8559, Test Acc: 0.7810, Test AUC: 0.8581\nEpoch: 17, Loss: 0.4746, Train Acc: 0.7810, Train AUC: 0.8559, Test Acc: 0.7815, Test AUC: 0.8582\nEpoch: 18, Loss: 0.4744, Train Acc: 0.7802, Train AUC: 0.8564, Test Acc: 0.7818, Test AUC: 0.8586\nEpoch: 19, Loss: 0.4741, Train Acc: 0.7813, Train AUC: 0.8545, Test Acc: 0.7833, Test AUC: 0.8563\nEpoch: 20, Loss: 0.4739, Train Acc: 0.7807, Train AUC: 0.8557, Test Acc: 0.7841, Test AUC: 0.8574\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Discussion of Results and Considerations\n\nThis section discusses the performance of the two chosen GNN architectures (EdgeConv and GAT) for quark/gluon jet classification, along with the key considerations taken during the implementation.\n\n**1. Graph Construction:**\n\n*   **k-Nearest Neighbors (k-NN) Graph (EdgeConv):**  We used a k-NN graph with `k=16`.  This choice was based on the idea that nearby particles in (eta, phi) space are more likely to be related.  The value of `k` is a hyperparameter; smaller values emphasize locality, while larger values incorporate more global context.  Experimentation with different `k` values would be beneficial.  We used Euclidean distance in (eta, phi) space, which is a physically meaningful metric for jets.\n\n*   **Fully Connected Graph (GAT):**  We used a fully connected graph, where every particle is connected to every other particle. This allows the model to learn relationships between all particles, regardless of their spatial proximity.  However, this is computationally expensive, making the attention mechanism in GAT essential.\n\n**2. Model Performance:**\n\n*   **EdgeConv:** The EdgeConv model, using the k-NN graph, provides a good baseline.  Its performance (as measured by test accuracy and AUC) is generally good, indicating that local relationships between particles are informative for distinguishing quarks and gluons.  The training and testing metrics track reasonably well, suggesting that the model is not severely overfitting.\n\n*   **GAT:** The GAT model, using the fully connected graph and attention, has the *potential* to outperform EdgeConv because it can learn more complex, global relationships. However, its performance can be sensitive to hyperparameters (e.g., the number of attention heads, hidden dimensions).  In practice, whether GAT outperforms EdgeConv significantly depends on the dataset and the tuning of these hyperparameters.  It's also typically more computationally expensive to train.\n\n**3. Key Considerations and Potential Improvements:**\n\n*   **Hyperparameter Tuning:**  The performance of both models could be improved by more extensive hyperparameter tuning.  This includes:\n    *   `k` for the k-NN graph.\n    *   The number of layers and hidden dimensions in both models.\n    *   The learning rate and optimizer (e.g., trying different optimizers like AdamW).\n    *   The number of attention heads in the GAT model.\n    *   The batch size.\n    *   The number of epochs.  Early stopping based on validation performance could prevent overfitting.\n\n*   **Data Augmentation:**  Data augmentation techniques, such as rotating or translating the jets in (eta, phi) space, could improve the model's robustness and generalization ability.\n\n*   **Edge Features:** We could incorporate edge features into the k-NN graph, such as the distance between connected particles or the difference in their features.  This could provide additional information to the EdgeConv model.\n\n*   **More Sophisticated Pooling:**  Instead of simple global mean pooling, we could explore more sophisticated pooling methods, such as attention-based pooling or hierarchical pooling.\n\n*   **Computational Resources:**  Training GNNs, especially GAT with a fully connected graph, can be computationally demanding.  Using a GPU is highly recommended.\n\n* **Number of Particles:** The choice of `num_particles = 100` is a trade-off between capturing enough information from the jets and computational cost.  Jets with fewer than 100 particles are padded, and those with more are truncated.  Experimenting with this parameter is crucial.\n\n* **Normalization:** Proper feature normalization is *critical* for good performance. We normalized each feature (pt, eta, phi, mass) separately *per jet*, which is important for preventing information leakage between jets.\n\n**4. Conclusion:**\n\nBoth EdgeConv and GAT are viable architectures for quark/gluon jet classification. EdgeConv provides a strong baseline by leveraging local relationships, while GAT offers the potential to capture more global context through attention. The choice between them depends on the specific dataset, computational resources, and the desired balance between performance and complexity. Further hyperparameter tuning and exploration of more advanced techniques could lead to significant improvements in classification accuracy. The results demonstrate the power of GNNs for analyzing particle physics data.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}