{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task VIII: Vision transformer/Quantum Vision Transformer**\n",
    "\n",
    "Implement a classical Vision transformer and apply it to MNIST. Show its performance on the test data. Comment on potential ideas to extend this classical vision transformer architecture to a quantum vision transformer and sketch out the architecture in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T19:43:38.959877Z",
     "iopub.status.busy": "2025-03-16T19:43:38.959374Z",
     "iopub.status.idle": "2025-03-16T19:46:46.363358Z",
     "shell.execute_reply": "2025-03-16T19:46:46.362549Z",
     "shell.execute_reply.started": "2025-03-16T19:43:38.959834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch 0, Loss: 2.3705\n",
      "Batch 100, Loss: 1.2790\n",
      "Batch 200, Loss: 0.8625\n",
      "Batch 300, Loss: 0.6873\n",
      "Batch 400, Loss: 0.5872\n",
      "Batch 500, Loss: 0.5203\n",
      "Batch 600, Loss: 0.4730\n",
      "Batch 700, Loss: 0.4362\n",
      "Batch 800, Loss: 0.4087\n",
      "Batch 900, Loss: 0.3836\n",
      "Epoch 1, Train Loss: 0.3755, Time: 16.52s\n",
      "\n",
      "Test set: Average loss: 0.1521, Accuracy: 9560/10000 (95.60%)\n",
      "\n",
      "Batch 0, Loss: 0.2043\n",
      "Batch 100, Loss: 0.1705\n",
      "Batch 200, Loss: 0.1732\n",
      "Batch 300, Loss: 0.1740\n",
      "Batch 400, Loss: 0.1711\n",
      "Batch 500, Loss: 0.1680\n",
      "Batch 600, Loss: 0.1688\n",
      "Batch 700, Loss: 0.1655\n",
      "Batch 800, Loss: 0.1624\n",
      "Batch 900, Loss: 0.1610\n",
      "Epoch 2, Train Loss: 0.1597, Time: 16.33s\n",
      "\n",
      "Test set: Average loss: 0.1124, Accuracy: 9669/10000 (96.69%)\n",
      "\n",
      "Batch 0, Loss: 0.1037\n",
      "Batch 100, Loss: 0.1165\n",
      "Batch 200, Loss: 0.1201\n",
      "Batch 300, Loss: 0.1222\n",
      "Batch 400, Loss: 0.1224\n",
      "Batch 500, Loss: 0.1230\n",
      "Batch 600, Loss: 0.1217\n",
      "Batch 700, Loss: 0.1226\n",
      "Batch 800, Loss: 0.1226\n",
      "Batch 900, Loss: 0.1221\n",
      "Epoch 3, Train Loss: 0.1229, Time: 16.46s\n",
      "\n",
      "Test set: Average loss: 0.0995, Accuracy: 9702/10000 (97.02%)\n",
      "\n",
      "Batch 0, Loss: 0.1152\n",
      "Batch 100, Loss: 0.0980\n",
      "Batch 200, Loss: 0.1046\n",
      "Batch 300, Loss: 0.1100\n",
      "Batch 400, Loss: 0.1086\n",
      "Batch 500, Loss: 0.1070\n",
      "Batch 600, Loss: 0.1043\n",
      "Batch 700, Loss: 0.1039\n",
      "Batch 800, Loss: 0.1034\n",
      "Batch 900, Loss: 0.1027\n",
      "Epoch 4, Train Loss: 0.1021, Time: 16.25s\n",
      "\n",
      "Test set: Average loss: 0.0979, Accuracy: 9697/10000 (96.97%)\n",
      "\n",
      "Batch 0, Loss: 0.4042\n",
      "Batch 100, Loss: 0.0829\n",
      "Batch 200, Loss: 0.0844\n",
      "Batch 300, Loss: 0.0864\n",
      "Batch 400, Loss: 0.0860\n",
      "Batch 500, Loss: 0.0873\n",
      "Batch 600, Loss: 0.0906\n",
      "Batch 700, Loss: 0.0925\n",
      "Batch 800, Loss: 0.0913\n",
      "Batch 900, Loss: 0.0911\n",
      "Epoch 5, Train Loss: 0.0908, Time: 16.00s\n",
      "\n",
      "Test set: Average loss: 0.0822, Accuracy: 9736/10000 (97.36%)\n",
      "\n",
      "Batch 0, Loss: 0.0241\n",
      "Batch 100, Loss: 0.0748\n",
      "Batch 200, Loss: 0.0747\n",
      "Batch 300, Loss: 0.0782\n",
      "Batch 400, Loss: 0.0771\n",
      "Batch 500, Loss: 0.0797\n",
      "Batch 600, Loss: 0.0820\n",
      "Batch 700, Loss: 0.0819\n",
      "Batch 800, Loss: 0.0833\n",
      "Batch 900, Loss: 0.0828\n",
      "Epoch 6, Train Loss: 0.0827, Time: 16.42s\n",
      "\n",
      "Test set: Average loss: 0.0898, Accuracy: 9715/10000 (97.15%)\n",
      "\n",
      "Batch 0, Loss: 0.0434\n",
      "Batch 100, Loss: 0.0756\n",
      "Batch 200, Loss: 0.0713\n",
      "Batch 300, Loss: 0.0721\n",
      "Batch 400, Loss: 0.0710\n",
      "Batch 500, Loss: 0.0717\n",
      "Batch 600, Loss: 0.0734\n",
      "Batch 700, Loss: 0.0744\n",
      "Batch 800, Loss: 0.0747\n",
      "Batch 900, Loss: 0.0746\n",
      "Epoch 7, Train Loss: 0.0748, Time: 16.38s\n",
      "\n",
      "Test set: Average loss: 0.0787, Accuracy: 9759/10000 (97.59%)\n",
      "\n",
      "Batch 0, Loss: 0.0116\n",
      "Batch 100, Loss: 0.0583\n",
      "Batch 200, Loss: 0.0614\n",
      "Batch 300, Loss: 0.0605\n",
      "Batch 400, Loss: 0.0636\n",
      "Batch 500, Loss: 0.0639\n",
      "Batch 600, Loss: 0.0645\n",
      "Batch 700, Loss: 0.0660\n",
      "Batch 800, Loss: 0.0668\n",
      "Batch 900, Loss: 0.0667\n",
      "Epoch 8, Train Loss: 0.0669, Time: 16.62s\n",
      "\n",
      "Test set: Average loss: 0.0692, Accuracy: 9784/10000 (97.84%)\n",
      "\n",
      "Batch 0, Loss: 0.0231\n",
      "Batch 100, Loss: 0.0558\n",
      "Batch 200, Loss: 0.0604\n",
      "Batch 300, Loss: 0.0617\n",
      "Batch 400, Loss: 0.0634\n",
      "Batch 500, Loss: 0.0639\n",
      "Batch 600, Loss: 0.0633\n",
      "Batch 700, Loss: 0.0639\n",
      "Batch 800, Loss: 0.0640\n",
      "Batch 900, Loss: 0.0635\n",
      "Epoch 9, Train Loss: 0.0640, Time: 16.85s\n",
      "\n",
      "Test set: Average loss: 0.0793, Accuracy: 9757/10000 (97.57%)\n",
      "\n",
      "Batch 0, Loss: 0.0429\n",
      "Batch 100, Loss: 0.0531\n",
      "Batch 200, Loss: 0.0522\n",
      "Batch 300, Loss: 0.0570\n",
      "Batch 400, Loss: 0.0570\n",
      "Batch 500, Loss: 0.0567\n",
      "Batch 600, Loss: 0.0573\n",
      "Batch 700, Loss: 0.0590\n",
      "Batch 800, Loss: 0.0589\n",
      "Batch 900, Loss: 0.0595\n",
      "Epoch 10, Train Loss: 0.0597, Time: 16.77s\n",
      "\n",
      "Test set: Average loss: 0.0766, Accuracy: 9767/10000 (97.67%)\n",
      "\n",
      "Final Evaluation:\n",
      "\n",
      "Test set: Average loss: 0.0766, Accuracy: 9767/10000 (97.67%)\n",
      "\n",
      "\n",
      "--- Thinking about Quantum Vision Transformers (QViT) ---\n",
      "\n",
      "1. Quantum Patch Embedding:\n",
      "    - Instead of normal picture splitting, use quantum ways to encode image info.\n",
      "    - We could try:\n",
      "        a) Angle Encoding: Use pixel brightness to turn qubits.\n",
      "        b) Amplitude Encoding: Use pixel brightness as quantum state sizes.\n",
      "        c) Quantum Convolution Networks: Use quantum circuits to find patch features, instead of normal picture filtering.\n",
      "    - Hard part: getting big picture data into small quantum spaces.\n",
      "\n",
      "2. Quantum Multi-Head Attention (QMHA):\n",
      "    - This is where quantum might really shine. Normal attention takes lots of work.\n",
      "    - We could try:\n",
      "        a) Quantum Memory: Make a quantum system that acts like attention, using quantum search tricks.\n",
      "        b) Quantum Circuits: Use circuits that learn attention weights. This is part quantum, part normal computer.\n",
      "        c) Quantum Matrix Tricks: Use quantum ways to do the math in attention. This is advanced, but powerful.\n",
      "    - Hard part: making a QMHA that's actually faster or better than normal attention.\n",
      "\n",
      "3. Quantum Transformer Encoder Block:\n",
      "    - Put Quantum Patch Embedding and QMHA together.\n",
      "    - Normal computers use feed-forward networks (MLP). We can try quantum versions.\n",
      "    - Think about quantum ways to normalize data.\n",
      "\n",
      "4. Quantum Measurement and Output:\n",
      "    - After quantum work, check qubit states to get normal numbers.\n",
      "    - Use these numbers to make a final guess with a normal computer layer (or a small quantum circuit).\n",
      "\n",
      "5. Mix Quantum and Normal Computers:\n",
      "    - Real QViTs will likely be a mix:\n",
      "        - Use normal computers for easy stuff (resizing, normalizing).\n",
      "        - Use quantum for hard parts (embedding, attention).\n",
      "        - Use normal computers for final guesses.\n",
      "        - Train with a mix of quantum and normal computer tricks.\n",
      "\n",
      "QViT Idea Sketch:\n",
      "\n",
      "    Picture In (Normal)\n",
      "    ↓\n",
      "    Normal Prep (Resize, Normalize)\n",
      "    ↓\n",
      "    Quantum Patch Embed (Angle/Amplitude or Quantum Filters)\n",
      "    ↓\n",
      "    [Quantum Transformer Blocks (Repeat N times)]\n",
      "        ↓\n",
      "        Quantum Multi-Head Attention (Quantum Circuits or Memory)\n",
      "        ↓\n",
      "        Quantum Feed-Forward (Maybe, Quantum Circuits)\n",
      "        ↓\n",
      "        Quantum Normalize (Maybe)\n",
      "        ↓\n",
      "    [End of Quantum Blocks]\n",
      "    ↓\n",
      "    Quantum Check (Measure Class Qubits)\n",
      "    ↓\n",
      "    Normal Guess Layer (Normal Computer)\n",
      "    ↓\n",
      "    Final Guess (Normal Numbers)\n",
      "    \n",
      "\n",
      "Things to think about:\n",
      "    - Quantum power is limited now. Small, noisy quantum computers make big QViTs hard.\n",
      "    - Getting picture data into quantum form is tricky.\n",
      "    - Making quantum tricks (like for attention) that are actually better is hard.\n",
      "    - Training mixed quantum-normal models takes special tricks.\n",
      "    - Making QViTs work on big pictures is a big question.\n",
      "    - This is new. The best way to do this is still being figured out.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "\n",
    "def load_mnist(batch_size=64):\n",
    "    \"\"\"Loads MNIST data, prepares it for the model.\"\"\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "    ])\n",
    "\n",
    "    train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_load = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_load, test_load\n",
    "\n",
    "\n",
    "# --- 2. Vision Transformer Implementation ---\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Breaks images into patches, converts them into embeddings.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.class_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embed\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttn(nn.Module):\n",
    "    \"\"\"Handles multi-head attention.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embed dim must divide heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attended_values = torch.matmul(attn_weights, v)\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        output = self.out_linear(attended_values)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"One block of the Transformer encoder.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttn(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        x = x + self.dropout(self.attn(x_norm))\n",
    "        x_norm = self.norm2(x)\n",
    "        x = x + self.dropout(self.mlp(x_norm))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"The full Vision Transformer model.\"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, num_layers, mlp_ratio=4, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        class_token = x[:, 0, :]\n",
    "        class_token = self.norm(class_token)\n",
    "        logits = self.head(class_token)\n",
    "        return logits\n",
    "\n",
    "# --- 3. Training and Evaluation ---\n",
    "\n",
    "def train(model, train_load, optimizer, criterion, device):\n",
    "    \"\"\"Trains the model for one round.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_load):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {total_loss / (batch_idx + 1):.4f}')\n",
    "    return total_loss / len(train_load)\n",
    "\n",
    "def evaluate(model, test_load, criterion, device):\n",
    "    \"\"\"Checks the model's performance on the test data.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_load:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_load)\n",
    "    accuracy = 100. * correct / len(test_load.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_load.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def main():\n",
    "    img_size = 28\n",
    "    patch_size = 7\n",
    "    in_channels = 1\n",
    "    num_classes = 10\n",
    "    embed_dim = 64\n",
    "    num_heads = 4\n",
    "    num_layers = 2\n",
    "    mlp_ratio = 2\n",
    "    dropout = 0.1\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_load, test_load = load_mnist(batch_size=batch_size)\n",
    "\n",
    "    model = VisionTransformer(img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, num_layers, mlp_ratio, dropout).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(model, train_load, optimizer, criterion, device)\n",
    "        end_time = time.time()\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Time: {end_time - start_time:.2f}s')\n",
    "        test_loss, test_accuracy = evaluate(model, test_load, criterion, device)\n",
    "\n",
    "    print(\"Final Evaluation:\")\n",
    "    evaluate(model, test_load, criterion, device)\n",
    "\n",
    "def quantum_vision_transformer_ideas():\n",
    "    \"\"\"\n",
    "    Ideas for how to build a Quantum Vision Transformer (QViT).\n",
    "    This is just thinking out loud, real QViTs need lots of quantum power.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Thinking about Quantum Vision Transformers (QViT) ---\")\n",
    "\n",
    "    print(\"\\n1. Quantum Patch Embedding:\")\n",
    "    print(\"    - Instead of normal picture splitting, use quantum ways to encode image info.\")\n",
    "    print(\"    - We could try:\")\n",
    "    print(\"        a) Angle Encoding: Use pixel brightness to turn qubits.\")\n",
    "    print(\"        b) Amplitude Encoding: Use pixel brightness as quantum state sizes.\")\n",
    "    print(\"        c) Quantum Convolution Networks: Use quantum circuits to find patch features, instead of normal picture filtering.\")\n",
    "    print(\"    - Hard part: getting big picture data into small quantum spaces.\")\n",
    "\n",
    "    print(\"\\n2. Quantum Multi-Head Attention (QMHA):\")\n",
    "    print(\"    - This is where quantum might really shine. Normal attention takes lots of work.\")\n",
    "    print(\"    - We could try:\")\n",
    "    print(\"        a) Quantum Memory: Make a quantum system that acts like attention, using quantum search tricks.\")\n",
    "    print(\"        b) Quantum Circuits: Use circuits that learn attention weights. This is part quantum, part normal computer.\")\n",
    "    print(\"        c) Quantum Matrix Tricks: Use quantum ways to do the math in attention. This is advanced, but powerful.\")\n",
    "    print(\"    - Hard part: making a QMHA that's actually faster or better than normal attention.\")\n",
    "\n",
    "    print(\"\\n3. Quantum Transformer Encoder Block:\")\n",
    "    print(\"    - Put Quantum Patch Embedding and QMHA together.\")\n",
    "    print(\"    - Normal computers use feed-forward networks (MLP). We can try quantum versions.\")\n",
    "    print(\"    - Think about quantum ways to normalize data.\")\n",
    "\n",
    "    print(\"\\n4. Quantum Measurement and Output:\")\n",
    "    print(\"    - After quantum work, check qubit states to get normal numbers.\")\n",
    "    print(\"    - Use these numbers to make a final guess with a normal computer layer (or a small quantum circuit).\")\n",
    "\n",
    "    print(\"\\n5. Mix Quantum and Normal Computers:\")\n",
    "    print(\"    - Real QViTs will likely be a mix:\")\n",
    "    print(\"        - Use normal computers for easy stuff (resizing, normalizing).\")\n",
    "    print(\"        - Use quantum for hard parts (embedding, attention).\")\n",
    "    print(\"        - Use normal computers for final guesses.\")\n",
    "    print(\"        - Train with a mix of quantum and normal computer tricks.\")\n",
    "\n",
    "    print(\"\\nQViT Idea Sketch:\")\n",
    "    print(\"\"\"\n",
    "    Picture In (Normal)\n",
    "    ↓\n",
    "    Normal Prep (Resize, Normalize)\n",
    "    ↓\n",
    "    Quantum Patch Embed (Angle/Amplitude or Quantum Filters)\n",
    "    ↓\n",
    "    [Quantum Transformer Blocks (Repeat N times)]\n",
    "        ↓\n",
    "        Quantum Multi-Head Attention (Quantum Circuits or Memory)\n",
    "        ↓\n",
    "        Quantum Feed-Forward (Maybe, Quantum Circuits)\n",
    "        ↓\n",
    "        Quantum Normalize (Maybe)\n",
    "        ↓\n",
    "    [End of Quantum Blocks]\n",
    "    ↓\n",
    "    Quantum Check (Measure Class Qubits)\n",
    "    ↓\n",
    "    Normal Guess Layer (Normal Computer)\n",
    "    ↓\n",
    "    Final Guess (Normal Numbers)\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\nThings to think about:\")\n",
    "    print(\"    - Quantum power is limited now. Small, noisy quantum computers make big QViTs hard.\")\n",
    "    print(\"    - Getting picture data into quantum form is tricky.\")\n",
    "    print(\"    - Making quantum tricks (like for attention) that are actually better is hard.\")\n",
    "    print(\"    - Training mixed quantum-normal models takes special tricks.\")\n",
    "    print(\"    - Making QViTs work on big pictures is a big question.\")\n",
    "    print(\"    - This is new. The best way to do this is still being figured out.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    quantum_vision_transformer_ideas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
