{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Task VIII: Vision transformer/Quantum Vision Transformer**\n\nImplement a classical Vision transformer and apply it to MNIST. Show its performance on the test data. Comment on potential ideas to extend this classical vision transformer architecture to a quantum vision transformer and sketch out the architecture in detail.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport time\n\n# --- 1. Data Loading and Preprocessing ---\n\ndef load_mnist(batch_size=64):\n    \"\"\"Loads MNIST data, prepares it for the model.\"\"\"\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n    ])\n\n    train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n    train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_load = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\n    return train_load, test_load\n\n\n# --- 2. Vision Transformer Implementation ---\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Breaks images into patches, converts them into embeddings.\"\"\"\n    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n        super(PatchEmbed, self).__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.embed_dim = embed_dim\n        self.n_patches = (img_size // patch_size) ** 2\n\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1, 2)\n        batch_size = x.shape[0]\n        cls_tokens = self.class_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embed\n        return x\n\nclass MultiHeadAttn(nn.Module):\n    \"\"\"Handles multi-head attention.\"\"\"\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttn, self).__init__()\n        assert embed_dim % num_heads == 0, \"Embed dim must divide heads\"\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n\n        self.q_linear = nn.Linear(embed_dim, embed_dim)\n        self.k_linear = nn.Linear(embed_dim, embed_dim)\n        self.v_linear = nn.Linear(embed_dim, embed_dim)\n        self.out_linear = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        batch_size, seq_len, embed_dim = x.size()\n\n        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attn_weights = torch.softmax(scores, dim=-1)\n        attended_values = torch.matmul(attn_weights, v)\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n        output = self.out_linear(attended_values)\n        return output\n\nclass TransformerBlock(nn.Module):\n    \"\"\"One block of the Transformer encoder.\"\"\"\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadAttn(embed_dim, num_heads)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n            nn.Dropout(dropout)\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x_norm = self.norm1(x)\n        x = x + self.dropout(self.attn(x_norm))\n        x_norm = self.norm2(x)\n        x = x + self.dropout(self.mlp(x_norm))\n        return x\n\nclass VisionTransformer(nn.Module):\n    \"\"\"The full Vision Transformer model.\"\"\"\n    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, num_layers, mlp_ratio=4, dropout=0.1):\n        super(VisionTransformer, self).__init__()\n\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n        self.transformer_blocks = nn.Sequential(\n            *[TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(num_layers)]\n        )\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.transformer_blocks(x)\n        class_token = x[:, 0, :]\n        class_token = self.norm(class_token)\n        logits = self.head(class_token)\n        return logits\n\n# --- 3. Training and Evaluation ---\n\ndef train(model, train_load, optimizer, criterion, device):\n    \"\"\"Trains the model for one round.\"\"\"\n    model.train()\n    total_loss = 0.0\n    for batch_idx, (data, target) in enumerate(train_load):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}, Loss: {total_loss / (batch_idx + 1):.4f}')\n    return total_loss / len(train_load)\n\ndef evaluate(model, test_load, criterion, device):\n    \"\"\"Checks the model's performance on the test data.\"\"\"\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_load:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_load)\n    accuracy = 100. * correct / len(test_load.dataset)\n    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_load.dataset)} ({accuracy:.2f}%)\\n')\n    return test_loss, accuracy\n\ndef main():\n    img_size = 28\n    patch_size = 7\n    in_channels = 1\n    num_classes = 10\n    embed_dim = 64\n    num_heads = 4\n    num_layers = 2\n    mlp_ratio = 2\n    dropout = 0.1\n    batch_size = 64\n    learning_rate = 0.001\n    num_epochs = 10\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    train_load, test_load = load_mnist(batch_size=batch_size)\n\n    model = VisionTransformer(img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, num_layers, mlp_ratio, dropout).to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(1, num_epochs + 1):\n        start_time = time.time()\n        train_loss = train(model, train_load, optimizer, criterion, device)\n        end_time = time.time()\n        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Time: {end_time - start_time:.2f}s')\n        test_loss, test_accuracy = evaluate(model, test_load, criterion, device)\n\n    print(\"Final Evaluation:\")\n    evaluate(model, test_load, criterion, device)\n\n# --- 4. Quantum Vision Transformer (Conceptual) ---\ndef quantum_vision_transformer_ideas():\n    \"\"\"\n    Ideas for how to build a Quantum Vision Transformer (QViT).\n    This is just thinking out loud, real QViTs need lots of quantum power.\n    \"\"\"\n    print(\"\\n--- Thinking about Quantum Vision Transformers (QViT) ---\")\n\n    print(\"\\n1. Quantum Patch Embedding:\")\n    print(\"    - Instead of normal picture splitting, use quantum ways to encode image info.\")\n    print(\"    - We could try:\")\n    print(\"        a) Angle Encoding: Use pixel brightness to turn qubits.\")\n    print(\"        b) Amplitude Encoding: Use pixel brightness as quantum state sizes.\")\n    print(\"        c) Quantum Convolution Networks: Use quantum circuits to find patch features, instead of normal picture filtering.\")\n    print(\"    - Hard part: getting big picture data into small quantum spaces.\")\n\n    print(\"\\n2. Quantum Multi-Head Attention (QMHA):\")\n    print(\"    - This is where quantum might really shine. Normal attention takes lots of work.\")\n    print(\"    - We could try:\")\n    print(\"        a) Quantum Memory: Make a quantum system that acts like attention, using quantum search tricks.\")\n    print(\"        b) Quantum Circuits: Use circuits that learn attention weights. This is part quantum, part normal computer.\")\n    print(\"        c) Quantum Matrix Tricks: Use quantum ways to do the math in attention. This is advanced, but powerful.\")\n    print(\"    - Hard part: making a QMHA that's actually faster or better than normal attention.\")\n\n    print(\"\\n3. Quantum Transformer Encoder Block:\")\n    print(\"    - Put Quantum Patch Embedding and QMHA together.\")\n    print(\"    - Normal computers use feed-forward networks (MLP). We can try quantum versions.\")\n    print(\"    - Think about quantum ways to normalize data.\")\n\n    print(\"\\n4. Quantum Measurement and Output:\")\n    print(\"    - After quantum work, check qubit states to get normal numbers.\")\n    print(\"    - Use these numbers to make a final guess with a normal computer layer (or a small quantum circuit).\")\n\n    print(\"\\n5. Mix Quantum and Normal Computers:\")\n    print(\"    - Real QViTs will likely be a mix:\")\n    print(\"        - Use normal computers for easy stuff (resizing, normalizing).\")\n    print(\"        - Use quantum for hard parts (embedding, attention).\")\n    print(\"        - Use normal computers for final guesses.\")\n    print(\"        - Train with a mix of quantum and normal computer tricks.\")\n\n    print(\"\\nQViT Idea Sketch:\")\n    print(\"\"\"\n    Picture In (Normal)\n    ↓\n    Normal Prep (Resize, Normalize)\n    ↓\n    Quantum Patch Embed (Angle/Amplitude or Quantum Filters)\n    ↓\n    [Quantum Transformer Blocks (Repeat N times)]\n        ↓\n        Quantum Multi-Head Attention (Quantum Circuits or Memory)\n        ↓\n        Quantum Feed-Forward (Maybe, Quantum Circuits)\n        ↓\n        Quantum Normalize (Maybe)\n        ↓\n    [End of Quantum Blocks]\n    ↓\n    Quantum Check (Measure Class Qubits)\n    ↓\n    Normal Guess Layer (Normal Computer)\n    ↓\n    Final Guess (Normal Numbers)\n    \"\"\")\n\n    print(\"\\nThings to think about:\")\n    print(\"    - Quantum power is limited now. Small, noisy quantum computers make big QViTs hard.\")\n    print(\"    - Getting picture data into quantum form is tricky.\")\n    print(\"    - Making quantum tricks (like for attention) that are actually better is hard.\")\n    print(\"    - Training mixed quantum-normal models takes special tricks.\")\n    print(\"    - Making QViTs work on big pictures is a big question.\")\n    print(\"    - This is new. The best way to do this is still being figured out.\")\n\nif __name__ == \"__main__\":\n    main()\n    quantum_vision_transformer_ideas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T19:43:38.959374Z","iopub.execute_input":"2025-03-16T19:43:38.959877Z","iopub.status.idle":"2025-03-16T19:46:46.363358Z","shell.execute_reply.started":"2025-03-16T19:43:38.959834Z","shell.execute_reply":"2025-03-16T19:46:46.362549Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nBatch 0, Loss: 2.3705\nBatch 100, Loss: 1.2790\nBatch 200, Loss: 0.8625\nBatch 300, Loss: 0.6873\nBatch 400, Loss: 0.5872\nBatch 500, Loss: 0.5203\nBatch 600, Loss: 0.4730\nBatch 700, Loss: 0.4362\nBatch 800, Loss: 0.4087\nBatch 900, Loss: 0.3836\nEpoch 1, Train Loss: 0.3755, Time: 16.52s\n\nTest set: Average loss: 0.1521, Accuracy: 9560/10000 (95.60%)\n\nBatch 0, Loss: 0.2043\nBatch 100, Loss: 0.1705\nBatch 200, Loss: 0.1732\nBatch 300, Loss: 0.1740\nBatch 400, Loss: 0.1711\nBatch 500, Loss: 0.1680\nBatch 600, Loss: 0.1688\nBatch 700, Loss: 0.1655\nBatch 800, Loss: 0.1624\nBatch 900, Loss: 0.1610\nEpoch 2, Train Loss: 0.1597, Time: 16.33s\n\nTest set: Average loss: 0.1124, Accuracy: 9669/10000 (96.69%)\n\nBatch 0, Loss: 0.1037\nBatch 100, Loss: 0.1165\nBatch 200, Loss: 0.1201\nBatch 300, Loss: 0.1222\nBatch 400, Loss: 0.1224\nBatch 500, Loss: 0.1230\nBatch 600, Loss: 0.1217\nBatch 700, Loss: 0.1226\nBatch 800, Loss: 0.1226\nBatch 900, Loss: 0.1221\nEpoch 3, Train Loss: 0.1229, Time: 16.46s\n\nTest set: Average loss: 0.0995, Accuracy: 9702/10000 (97.02%)\n\nBatch 0, Loss: 0.1152\nBatch 100, Loss: 0.0980\nBatch 200, Loss: 0.1046\nBatch 300, Loss: 0.1100\nBatch 400, Loss: 0.1086\nBatch 500, Loss: 0.1070\nBatch 600, Loss: 0.1043\nBatch 700, Loss: 0.1039\nBatch 800, Loss: 0.1034\nBatch 900, Loss: 0.1027\nEpoch 4, Train Loss: 0.1021, Time: 16.25s\n\nTest set: Average loss: 0.0979, Accuracy: 9697/10000 (96.97%)\n\nBatch 0, Loss: 0.4042\nBatch 100, Loss: 0.0829\nBatch 200, Loss: 0.0844\nBatch 300, Loss: 0.0864\nBatch 400, Loss: 0.0860\nBatch 500, Loss: 0.0873\nBatch 600, Loss: 0.0906\nBatch 700, Loss: 0.0925\nBatch 800, Loss: 0.0913\nBatch 900, Loss: 0.0911\nEpoch 5, Train Loss: 0.0908, Time: 16.00s\n\nTest set: Average loss: 0.0822, Accuracy: 9736/10000 (97.36%)\n\nBatch 0, Loss: 0.0241\nBatch 100, Loss: 0.0748\nBatch 200, Loss: 0.0747\nBatch 300, Loss: 0.0782\nBatch 400, Loss: 0.0771\nBatch 500, Loss: 0.0797\nBatch 600, Loss: 0.0820\nBatch 700, Loss: 0.0819\nBatch 800, Loss: 0.0833\nBatch 900, Loss: 0.0828\nEpoch 6, Train Loss: 0.0827, Time: 16.42s\n\nTest set: Average loss: 0.0898, Accuracy: 9715/10000 (97.15%)\n\nBatch 0, Loss: 0.0434\nBatch 100, Loss: 0.0756\nBatch 200, Loss: 0.0713\nBatch 300, Loss: 0.0721\nBatch 400, Loss: 0.0710\nBatch 500, Loss: 0.0717\nBatch 600, Loss: 0.0734\nBatch 700, Loss: 0.0744\nBatch 800, Loss: 0.0747\nBatch 900, Loss: 0.0746\nEpoch 7, Train Loss: 0.0748, Time: 16.38s\n\nTest set: Average loss: 0.0787, Accuracy: 9759/10000 (97.59%)\n\nBatch 0, Loss: 0.0116\nBatch 100, Loss: 0.0583\nBatch 200, Loss: 0.0614\nBatch 300, Loss: 0.0605\nBatch 400, Loss: 0.0636\nBatch 500, Loss: 0.0639\nBatch 600, Loss: 0.0645\nBatch 700, Loss: 0.0660\nBatch 800, Loss: 0.0668\nBatch 900, Loss: 0.0667\nEpoch 8, Train Loss: 0.0669, Time: 16.62s\n\nTest set: Average loss: 0.0692, Accuracy: 9784/10000 (97.84%)\n\nBatch 0, Loss: 0.0231\nBatch 100, Loss: 0.0558\nBatch 200, Loss: 0.0604\nBatch 300, Loss: 0.0617\nBatch 400, Loss: 0.0634\nBatch 500, Loss: 0.0639\nBatch 600, Loss: 0.0633\nBatch 700, Loss: 0.0639\nBatch 800, Loss: 0.0640\nBatch 900, Loss: 0.0635\nEpoch 9, Train Loss: 0.0640, Time: 16.85s\n\nTest set: Average loss: 0.0793, Accuracy: 9757/10000 (97.57%)\n\nBatch 0, Loss: 0.0429\nBatch 100, Loss: 0.0531\nBatch 200, Loss: 0.0522\nBatch 300, Loss: 0.0570\nBatch 400, Loss: 0.0570\nBatch 500, Loss: 0.0567\nBatch 600, Loss: 0.0573\nBatch 700, Loss: 0.0590\nBatch 800, Loss: 0.0589\nBatch 900, Loss: 0.0595\nEpoch 10, Train Loss: 0.0597, Time: 16.77s\n\nTest set: Average loss: 0.0766, Accuracy: 9767/10000 (97.67%)\n\nFinal Evaluation:\n\nTest set: Average loss: 0.0766, Accuracy: 9767/10000 (97.67%)\n\n\n--- Thinking about Quantum Vision Transformers (QViT) ---\n\n1. Quantum Patch Embedding:\n    - Instead of normal picture splitting, use quantum ways to encode image info.\n    - We could try:\n        a) Angle Encoding: Use pixel brightness to turn qubits.\n        b) Amplitude Encoding: Use pixel brightness as quantum state sizes.\n        c) Quantum Convolution Networks: Use quantum circuits to find patch features, instead of normal picture filtering.\n    - Hard part: getting big picture data into small quantum spaces.\n\n2. Quantum Multi-Head Attention (QMHA):\n    - This is where quantum might really shine. Normal attention takes lots of work.\n    - We could try:\n        a) Quantum Memory: Make a quantum system that acts like attention, using quantum search tricks.\n        b) Quantum Circuits: Use circuits that learn attention weights. This is part quantum, part normal computer.\n        c) Quantum Matrix Tricks: Use quantum ways to do the math in attention. This is advanced, but powerful.\n    - Hard part: making a QMHA that's actually faster or better than normal attention.\n\n3. Quantum Transformer Encoder Block:\n    - Put Quantum Patch Embedding and QMHA together.\n    - Normal computers use feed-forward networks (MLP). We can try quantum versions.\n    - Think about quantum ways to normalize data.\n\n4. Quantum Measurement and Output:\n    - After quantum work, check qubit states to get normal numbers.\n    - Use these numbers to make a final guess with a normal computer layer (or a small quantum circuit).\n\n5. Mix Quantum and Normal Computers:\n    - Real QViTs will likely be a mix:\n        - Use normal computers for easy stuff (resizing, normalizing).\n        - Use quantum for hard parts (embedding, attention).\n        - Use normal computers for final guesses.\n        - Train with a mix of quantum and normal computer tricks.\n\nQViT Idea Sketch:\n\n    Picture In (Normal)\n    ↓\n    Normal Prep (Resize, Normalize)\n    ↓\n    Quantum Patch Embed (Angle/Amplitude or Quantum Filters)\n    ↓\n    [Quantum Transformer Blocks (Repeat N times)]\n        ↓\n        Quantum Multi-Head Attention (Quantum Circuits or Memory)\n        ↓\n        Quantum Feed-Forward (Maybe, Quantum Circuits)\n        ↓\n        Quantum Normalize (Maybe)\n        ↓\n    [End of Quantum Blocks]\n    ↓\n    Quantum Check (Measure Class Qubits)\n    ↓\n    Normal Guess Layer (Normal Computer)\n    ↓\n    Final Guess (Normal Numbers)\n    \n\nThings to think about:\n    - Quantum power is limited now. Small, noisy quantum computers make big QViTs hard.\n    - Getting picture data into quantum form is tricky.\n    - Making quantum tricks (like for attention) that are actually better is hard.\n    - Training mixed quantum-normal models takes special tricks.\n    - Making QViTs work on big pictures is a big question.\n    - This is new. The best way to do this is still being figured out.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}