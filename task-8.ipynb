{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Task VIII: Vision transformer/Quantum Vision Transformer**\n\nImplement a classical Vision transformer and apply it to MNIST. Show its performance on the test data. Comment on potential ideas to extend this classical vision transformer architecture to a quantum vision transformer and sketch out the architecture in detail.","metadata":{}},{"cell_type":"code","source":"!pip install pennylane\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pennylane as qml\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# For reproducible results, set random number generator seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Determine if CUDA (GPU) is available, otherwise use CPU\ncomputing_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {computing_device}\")\n\n# Create a custom dataset that mimics MNIST for demonstration\nclass SyntheticMNISTDataset(Dataset):\n    def __init__(self, num_samples=10000, is_train_set=True):\n        \"\"\"\n        Initializes the Synthetic MNIST-like dataset.\n\n        Args:\n            num_samples (int): Total number of samples in the dataset.\n            is_train_set (bool): True for training set, False for test set.\n        \"\"\"\n        self.num_samples = num_samples\n        self.is_train_set = is_train_set\n\n        # Initialize empty arrays for images and labels\n        self.images = np.zeros((num_samples, 28, 28), dtype=np.float32)\n        self.labels = np.zeros(num_samples, dtype=np.int64)\n\n        # Define basic patterns for digits 0 to 9\n        digit_patterns = []\n        for digit_value in range(10):\n            # Initialize a blank 28x28 pattern\n            pattern = np.zeros((28, 28), dtype=np.float32)\n\n            # Define simple shapes for each digit\n            if digit_value == 0:  # Circle\n                for x_pixel in range(28):\n                    for y_pixel in range(28):\n                        if 8 < x_pixel < 20 and 8 < y_pixel < 20:\n                            dx_circle, dy_circle = x_pixel - 14, y_pixel - 14\n                            distance_from_center = np.sqrt(dx_circle**2 + dy_circle**2)\n                            if 4 < distance_from_center < 6:\n                                pattern[x_pixel, y_pixel] = 1.0\n            elif digit_value == 1:  # Vertical line\n                pattern[5:23, 13:15] = 1.0\n            elif digit_value == 2:  # Horizontal zigzag\n                for x_pixel in range(6, 22, 4):\n                    pattern[x_pixel:x_pixel+4, 8:20] = 1.0\n            elif digit_value == 3:  # Cross\n                pattern[10:18, 8:20] = 1.0\n                pattern[6:22, 13:15] = 1.0\n            elif digit_value == 4:  # Square\n                pattern[8:20, 8:20] = 1.0\n                pattern[10:18, 10:18] = 0.0\n            elif digit_value == 5:  # Diamond\n                for x_pixel in range(28):\n                    for y_pixel in range(28):\n                        if abs(x_pixel - 14) + abs(y_pixel - 14) < 8:\n                            pattern[x_pixel, y_pixel] = 1.0\n            elif digit_value == 6:  # Plus\n                pattern[13:15, 8:20] = 1.0\n                pattern[8:20, 13:15] = 1.0\n            elif digit_value == 7:  # T-shape\n                pattern[8:10, 8:20] = 1.0\n                pattern[10:22, 13:15] = 1.0\n            elif digit_value == 8:  # Eight (two circles)\n                for x_pixel in range(28):\n                    for y_pixel in range(28):\n                        if 8 < x_pixel < 20:\n                            dy_circle1 = y_pixel - 10\n                            dy_circle2 = y_pixel - 18\n                            dx_circle = x_pixel - 14\n                            distance_circle1 = np.sqrt(dx_circle**2 + dy_circle1**2)\n                            distance_circle2 = np.sqrt(dx_circle**2 + dy_circle2**2)\n                            if distance_circle1 < 4 or distance_circle2 < 4:\n                                pattern[x_pixel, y_pixel] = 1.0\n            elif digit_value == 9:  # Nine (circle with tail)\n                for x_pixel in range(28):\n                    for y_pixel in range(28):\n                        if 8 < x_pixel < 20 and 8 < y_pixel < 16:\n                            dx_circle, dy_circle = x_pixel - 14, y_pixel - 12\n                            distance_from_center = np.sqrt(dx_circle**2 + dy_circle**2)\n                            if distance_from_center < 4:\n                                pattern[x_pixel, y_pixel] = 1.0\n                    pattern[14:16, 16:22] = 1.0\n\n            digit_patterns.append(pattern)\n\n        # Generate dataset samples\n        samples_per_digit_class = num_samples // 10\n        for digit_value in range(10):\n            start_index = digit_value * samples_per_digit_class\n            end_index = (digit_value + 1) * samples_per_digit_class\n\n            for sample_index in range(start_index, end_index):\n                # Introduce slight variations to each pattern\n                noise = np.random.normal(0, 0.1, (28, 28))\n                horizontal_shift = np.random.randint(-2, 3)\n                vertical_shift = np.random.randint(-2, 3)\n\n                # Shift and add noise to the base pattern\n                image_instance = np.roll(np.roll(digit_patterns[digit_value], horizontal_shift, axis=0), vertical_shift, axis=1)\n                image_instance = np.clip(image_instance + noise, 0, 1)\n\n                self.images[sample_index] = image_instance\n                self.labels[sample_index] = digit_value\n\n        # Randomly shuffle the generated dataset\n        shuffle_indices = np.random.permutation(num_samples)\n        self.images = self.images[shuffle_indices]\n        self.labels = self.labels[shuffle_indices]\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n        return self.num_samples\n\n    def __getitem__(self, index):\n        \"\"\"\n        Retrieves an image and its label given an index.\n\n        Args:\n            index (int): Index of the sample to retrieve.\n\n        Returns:\n            tuple: (image, label) where image is a tensor and label is an integer.\n        \"\"\"\n        image = torch.FloatTensor(self.images[index]).unsqueeze(0)  # Add channel dimension for grayscale\n        label = self.labels[index]\n        return image, label\n\n# Create training and testing datasets\ntraining_dataset = SyntheticMNISTDataset(num_samples=5000, is_train_set=True)\ntesting_dataset = SyntheticMNISTDataset(num_samples=1000, is_train_set=False)\n\n# Define hyperparameters for training\ntraining_batch_size = 64\nnum_training_epochs = 3  # Reduced epochs for quicker demonstration\nlearning_rate = 0.001\n\n# Create data loaders for training and testing\ntraining_dataloader = DataLoader(training_dataset, batch_size=training_batch_size, shuffle=True)\ntesting_dataloader = DataLoader(testing_dataset, batch_size=training_batch_size, shuffle=False)\n\n# Vision Transformer Model Parameters\nimage_resolution = 28  # Input image size: 28x28\npatch_resolution = 7  # Divide image into 7x7 patches\nnum_image_patches = (image_resolution // patch_resolution) ** 2  # Total number of patches\nembedding_dimension = 64  # Dimension of patch embeddings\nnum_attention_heads = 4  # Number of attention heads in Transformer\ntransformer_depth = 2  # Number of Transformer encoder blocks\nmlp_hidden_dimension = 128  # Hidden dimension in MLP layers\nnum_output_classes = 10  # 10 classes for digits 0-9\ninput_channels = 1  # Grayscale images have 1 channel\n\n# Define the Patch Embedding Layer\nclass PatchEmbeddingLayer(nn.Module):\n    def __init__(self, image_resolution, patch_resolution, input_channels, embedding_dimension):\n        \"\"\"\n        Converts input image patches into embeddings.\n\n        Args:\n            image_resolution (int): Height/Width of the input image.\n            patch_resolution (int): Height/Width of each patch.\n            input_channels (int): Number of input channels (e.g., 1 for grayscale, 3 for RGB).\n            embedding_dimension (int): Dimensionality of the patch embeddings.\n        \"\"\"\n        super().__init__()\n        self.image_resolution = image_resolution\n        self.patch_resolution = patch_resolution\n        self.num_image_patches = (image_resolution // patch_resolution) ** 2\n\n        # Convolutional layer to create patch embeddings\n        self.patch_projection = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=embedding_dimension,\n            kernel_size=patch_resolution,\n            stride=patch_resolution\n        )\n\n    def forward(self, input_images):\n        \"\"\"\n        Forward pass of the patch embedding layer.\n\n        Args:\n            input_images (torch.Tensor): Input images of shape (B, C, H, W).\n\n        Returns:\n            torch.Tensor: Patch embeddings of shape (B, num_patches, embedding_dimension).\n        \"\"\"\n        # Input shape: (Batch Size, Channels, Height, Width)\n        patches = self.patch_projection(input_images)  # Shape: (B, embedding_dimension, H/patch_size, W/patch_size)\n        patches_flattened = patches.flatten(2)  # Shape: (B, embedding_dimension, num_patches)\n        patches_embedded = patches_flattened.transpose(1, 2)  # Shape: (B, num_patches, embedding_dimension)\n        return patches_embedded\n\n# Define a Transformer Encoder Block\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, embedding_dimension, num_attention_heads, mlp_hidden_dimension, dropout_rate=0.1):\n        \"\"\"\n        Transformer encoder block with multi-head attention and MLP.\n\n        Args:\n            embedding_dimension (int): Embedding dimension.\n            num_attention_heads (int): Number of attention heads.\n            mlp_hidden_dimension (int): Hidden dimension in MLP.\n            dropout_rate (float): Dropout probability.\n        \"\"\"\n        super().__init__()\n        self.layer_norm_1 = nn.LayerNorm(embedding_dimension)\n        self.multi_head_attention = nn.MultiheadAttention(embedding_dimension, num_attention_heads)\n        self.layer_norm_2 = nn.LayerNorm(embedding_dimension)\n        self.mlp_feedforward = nn.Sequential(\n            nn.Linear(embedding_dimension, mlp_hidden_dimension),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dimension, embedding_dimension),\n            nn.Dropout(dropout_rate)\n        )\n\n    def forward(self, input_embeddings):\n        \"\"\"\n        Forward pass of the Transformer encoder block.\n\n        Args:\n            input_embeddings (torch.Tensor): Input embeddings.\n\n        Returns:\n            torch.Tensor: Output embeddings after transformer block.\n        \"\"\"\n        # Input shape: (B, num_patches + 1, embedding_dimension)\n        # Multi-head Self-Attention\n        normalized_input = self.layer_norm_1(input_embeddings)\n        attention_output, _ = self.multi_head_attention(normalized_input.transpose(0, 1), normalized_input.transpose(0, 1), normalized_input.transpose(0, 1))\n        attention_output = attention_output.transpose(0, 1)  # Shape: (B, num_patches + 1, embedding_dimension)\n        embeddings_after_attention = input_embeddings + attention_output\n\n        # Feed-forward MLP\n        embeddings_after_norm = self.layer_norm_2(embeddings_after_attention)\n        mlp_output = self.mlp_feedforward(embeddings_after_norm)\n        output_embeddings = embeddings_after_attention + mlp_output\n        return output_embeddings\n\n# Define the Vision Transformer (ViT) Model\nclass VisionTransformerModel(nn.Module):\n    def __init__(self, image_resolution, patch_resolution, input_channels, embedding_dimension, num_attention_heads, transformer_depth, mlp_hidden_dimension, num_output_classes):\n        \"\"\"\n        Vision Transformer (ViT) model for image classification.\n\n        Args:\n            image_resolution (int): Height/Width of the input image.\n            patch_resolution (int): Height/Width of each patch.\n            input_channels (int): Number of input channels.\n            embedding_dimension (int): Embedding dimension for patches.\n            num_attention_heads (int): Number of attention heads in Transformer.\n            transformer_depth (int): Number of Transformer encoder blocks.\n            mlp_hidden_dimension (int): Hidden dimension in MLP layers.\n            num_output_classes (int): Number of output classes.\n        \"\"\"\n        super().__init__()\n        self.patch_embed = PatchEmbeddingLayer(image_resolution, patch_resolution, input_channels, embedding_dimension)\n        self.num_image_patches = self.patch_embed.num_image_patches\n\n        # Learnable class token and positional embeddings\n        self.class_token = nn.Parameter(torch.randn(1, 1, embedding_dimension))\n        self.position_embeddings = nn.Parameter(torch.randn(1, self.num_image_patches + 1, embedding_dimension))\n\n        # Stack of Transformer encoder blocks\n        self.transformer_encoder_layers = nn.ModuleList([\n            TransformerEncoderBlock(embedding_dimension, num_attention_heads, mlp_hidden_dimension)\n            for _ in range(transformer_depth)\n        ])\n\n        self.layer_norm_final = nn.LayerNorm(embedding_dimension)\n        self.classification_head = nn.Linear(embedding_dimension, num_output_classes)\n\n    def forward(self, input_images):\n        \"\"\"\n        Forward pass of the Vision Transformer model.\n\n        Args:\n            input_images (torch.Tensor): Input images of shape (B, C, H, W).\n\n        Returns:\n            torch.Tensor: Classification output logits of shape (B, num_output_classes).\n        \"\"\"\n        # Input shape: (Batch Size, Channels, Height, Width)\n        batch_size = input_images.shape[0]\n\n        # Create patch embeddings\n        patches_embedded = self.patch_embed(input_images)  # Shape: (B, num_patches, embedding_dimension)\n\n        # Prepend class token to patch embeddings\n        class_tokens = self.class_token.expand(batch_size, -1, -1)  # Shape: (B, 1, embedding_dimension)\n        embeddings_with_class_token = torch.cat([class_tokens, patches_embedded], dim=1)  # Shape: (B, num_patches + 1, embedding_dimension)\n\n        # Add positional embeddings\n        transformer_input = embeddings_with_class_token + self.position_embeddings\n\n        # Pass through Transformer encoder blocks\n        transformer_output = transformer_input\n        for transformer_block in self.transformer_encoder_layers:\n            transformer_output = transformer_block(transformer_output)\n\n        # Extract class token embedding and normalize\n        class_token_embedding = self.layer_norm_final(transformer_output[:, 0])  # Shape: (B, embedding_dimension)\n\n        # Classification head for final output\n        output_logits = self.classification_head(class_token_embedding)  # Shape: (B, num_output_classes)\n        return output_logits\n\n# Initialize the Vision Transformer model\nvision_transformer_model = VisionTransformerModel(\n    image_resolution=image_resolution,\n    patch_resolution=patch_resolution,\n    input_channels=input_channels,\n    embedding_dimension=embedding_dimension,\n    num_attention_heads=num_attention_heads,\n    transformer_depth=transformer_depth,\n    mlp_hidden_dimension=mlp_hidden_dimension,\n    num_output_classes=num_output_classes\n).to(computing_device)\n\nprint(f\"Vision Transformer initialized with {sum(p.numel() for p in vision_transformer_model.parameters())} parameters\")\n\n# Loss function and optimizer for training\nloss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vision_transformer_model.parameters(), lr=learning_rate)\n\n# Training loop\ntotal_steps_per_epoch = len(training_dataloader)\ntraining_losses = []\nprint(\"Starting training process...\")\nfor epoch in range(num_training_epochs):\n    vision_transformer_model.train()  # Set model to training mode\n    current_running_loss = 0.0\n    epoch_start_time = time.time()\n    for step_index, (images, labels) in enumerate(training_dataloader):\n        images = images.to(computing_device)\n        labels = labels.to(computing_device)\n\n        # Forward pass\n        outputs = vision_transformer_model(images)\n        loss = loss_criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()  # Clear gradients from previous step\n        loss.backward()  # Compute gradients\n        optimizer.step()  # Update model parameters\n\n        current_running_loss += loss.item()\n\n        if (step_index + 1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_training_epochs}], Step [{step_index+1}/{total_steps_per_epoch}], Loss: {loss.item():.4f}')\n\n    epoch_loss = current_running_loss / len(training_dataloader)\n    training_losses.append(epoch_loss)\n    epoch_end_time = time.time()\n    print(f'Epoch [{epoch+1}/{num_training_epochs}], Loss: {epoch_loss:.4f}, Time: {epoch_end_time - epoch_start_time:.2f}s')\n\n# Evaluate the trained model on the test dataset\nvision_transformer_model.eval()  # Set model to evaluation mode\nwith torch.no_grad():  # Disable gradient calculation during inference\n    num_correct_predictions = 0\n    total_samples = 0\n    for images, labels in testing_dataloader:\n        images = images.to(computing_device)\n        labels = labels.to(computing_device)\n        outputs = vision_transformer_model(images)\n        _, predicted_labels = torch.max(outputs.data, 1)  # Get index of the max log-probability\n        total_samples += labels.size(0)\n        num_correct_predictions += (predicted_labels == labels).sum().item()\n\ntest_accuracy = 100 * num_correct_predictions / total_samples\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n\n# Plot the training loss curve\nplt.figure(figsize=(10, 5))\nplt.plot(training_losses)\nplt.title('Training Loss Curve')\nplt.xlabel('Epoch Number')\nplt.ylabel('Training Loss')\nplt.grid(True)\nplt.savefig('training_loss.png')\nplt.close()\n\n# ===================================\n# Quantum Vision Transformer Architecture\n# ===================================\n\n# Define a quantum computing device using PennyLane simulator\nnum_quantum_qubits = 4  # Number of qubits for quantum components\nquantum_device = qml.device(\"default.qubit\", wires=num_quantum_qubits)\nprint(f\"Quantum device initialized with {num_quantum_qubits} qubits\")\n\n# Define a quantum circuit for feature extraction\n@qml.qnode(quantum_device)\ndef quantum_feature_circuit(input_features, quantum_weights):\n    \"\"\"\n    Quantum circuit for feature extraction using angle embedding and strongly entangling layers.\n\n    Args:\n        input_features (list[float]): Classical input features to be encoded.\n        quantum_weights (array): Trainable parameters for quantum layers.\n\n    Returns:\n        list[float]: Expectation values of PauliZ operators for each qubit.\n    \"\"\"\n    # Encode classical inputs into quantum state amplitudes\n    qml.templates.AngleEmbedding(input_features, wires=range(num_quantum_qubits))\n\n    # Apply parameterized quantum operations (layers of gates)\n    qml.templates.StronglyEntanglingLayers(quantum_weights, wires=range(num_quantum_qubits))\n\n    # Measure expectation value of Pauli Z on each qubit\n    return [qml.expval(qml.PauliZ(qubit_index)) for qubit_index in range(num_quantum_qubits)]\n\n# Quantum-enhanced Multi-Head Attention Mechanism\nclass QuantumAttentionLayer(nn.Module):\n    def __init__(self, embedding_dimension, num_attention_heads, num_quantum_qubits=4, dropout_rate=0.1):\n        \"\"\"\n        Multi-head attention layer enhanced with quantum processing in the query part.\n\n        Args:\n            embedding_dimension (int): Embedding dimension of inputs.\n            num_attention_heads (int): Number of attention heads.\n            num_quantum_qubits (int): Number of qubits to use in quantum circuit.\n            dropout_rate (float): Dropout probability.\n        \"\"\"\n        super().__init__()\n        self.embedding_dimension = embedding_dimension\n        self.num_attention_heads = num_attention_heads\n        self.num_quantum_qubits = num_quantum_qubits\n        self.head_dimension = embedding_dimension // num_attention_heads\n\n        # Quantum circuit parameters - initialized randomly\n        self.quantum_circuit_weights = nn.Parameter(\n            torch.FloatTensor(2, num_quantum_qubits, 3).uniform_(0, 2 * np.pi)\n        )\n\n        # Linear layers for projecting Query, Key, Value\n        self.query_projection = nn.Linear(embedding_dimension, embedding_dimension)\n        self.key_projection = nn.Linear(embedding_dimension, embedding_dimension)\n        self.value_projection = nn.Linear(embedding_dimension, embedding_dimension)\n        self.output_projection = nn.Linear(embedding_dimension, embedding_dimension)\n\n        self.dropout_layer = nn.Dropout(dropout_rate)\n\n    def forward(self, input_embeddings):\n        \"\"\"\n        Forward pass of the Quantum Attention Layer.\n\n        Args:\n            input_embeddings (torch.Tensor): Input embeddings.\n\n        Returns:\n            torch.Tensor: Output embeddings after quantum-enhanced attention.\n        \"\"\"\n        batch_size_val = input_embeddings.size(0)\n        sequence_length = input_embeddings.size(1)\n\n        # Project input into Query, Key, and Value spaces\n        query_layer = self.query_projection(input_embeddings)\n        key_layer = self.key_projection(input_embeddings)\n        value_layer = self.value_projection(input_embeddings)\n\n        # Prepare Query for quantum processing\n        quantum_feature_dimension = min(self.num_quantum_qubits, self.head_dimension) # Use at most num_quantum_qubits features for quantum processing\n        query_reshaped = query_layer.view(batch_size_val * sequence_length, self.num_attention_heads, self.head_dimension)\n\n        # Apply quantum circuit to a subset of query features\n        query_quantum_enhanced = query_reshaped.clone() # Initialize tensor to store quantum processed queries\n\n        # Example: Process a small batch and heads for demonstration speed\n        sample_batch_size = min(batch_size_val * sequence_length, 10) # Limit batch size for faster execution\n        sample_heads_count = min(self.num_attention_heads, 2) # Limit head count for faster execution\n\n        for batch_index in range(sample_batch_size):\n            for head_index in range(sample_heads_count):\n                # Extract features for quantum circuit\n                features_to_process = query_reshaped[batch_index, head_index, :quantum_feature_dimension].detach().cpu().numpy()\n\n                # Scale features to range [0, 2pi] for angle encoding\n                scaled_features = (features_to_process * 0.5 + 0.5) * 2 * np.pi\n\n                # Execute quantum circuit to enhance query\n                circuit_weights = self.quantum_circuit_weights.detach().cpu().numpy()\n                quantum_results = torch.tensor(quantum_feature_circuit(scaled_features, circuit_weights))\n\n                # Update query with quantum results\n                query_quantum_enhanced[batch_index, head_index, :quantum_feature_dimension] = quantum_results\n\n        # Reshape query back to original format\n        query_layer = query_quantum_enhanced.view(batch_size_val, sequence_length, -1)\n\n        # Standard Multi-Head Attention calculation\n        query_heads = query_layer.view(batch_size_val, sequence_length, self.num_attention_heads, self.head_dimension).transpose(1, 2)\n        key_heads = key_layer.view(batch_size_val, sequence_length, self.num_attention_heads, self.head_dimension).transpose(1, 2)\n        value_heads = value_layer.view(batch_size_val, sequence_length, self.num_attention_heads, self.head_dimension).transpose(1, 2)\n\n        # Calculate attention scores\n        attention_scores = torch.matmul(query_heads, key_heads.transpose(-2, -1)) / (self.head_dimension ** 0.5)\n\n        # Apply softmax to get attention probabilities\n        attention_probabilities = torch.softmax(attention_scores, dim=-1)\n        attention_probabilities = self.dropout_layer(attention_probabilities)\n\n        # Compute context vector (weighted sum of values)\n        attention_output_heads = torch.matmul(attention_probabilities, value_heads)\n        attention_output_merged = attention_output_heads.transpose(1, 2).reshape(batch_size_val, sequence_length, self.embedding_dimension)\n        attention_output_projected = self.output_projection(attention_output_merged)\n\n        return attention_output_projected\n\n# Quantum Transformer Encoder Block\nclass QuantumTransformerEncoderBlock(nn.Module):\n    def __init__(self, embedding_dimension, num_attention_heads, mlp_hidden_dimension, num_quantum_qubits=4, dropout_rate=0.1):\n        \"\"\"\n        Transformer encoder block with quantum-enhanced multi-head attention.\n\n        Args:\n            embedding_dimension (int): Embedding dimension.\n            num_attention_heads (int): Number of attention heads.\n            mlp_hidden_dimension (int): Hidden dimension in MLP.\n            num_quantum_qubits (int): Number of qubits for quantum attention.\n            dropout_rate (float): Dropout probability.\n        \"\"\"\n        super().__init__()\n        self.layer_norm_1 = nn.LayerNorm(embedding_dimension)\n        self.quantum_attention = QuantumAttentionLayer(embedding_dimension, num_attention_heads, num_quantum_qubits, dropout_rate) # Use quantum attention here\n        self.layer_norm_2 = nn.LayerNorm(embedding_dimension)\n        # MLP remains classical for this hybrid approach\n        self.mlp_feedforward = nn.Sequential(\n            nn.Linear(embedding_dimension, mlp_hidden_dimension),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(mlp_hidden_dimension, embedding_dimension),\n            nn.Dropout(dropout_rate)\n        )\n\n    def forward(self, input_embeddings):\n        \"\"\"\n        Forward pass of the Quantum Transformer encoder block.\n\n        Args:\n            input_embeddings (torch.Tensor): Input embeddings.\n\n        Returns:\n            torch.Tensor: Output embeddings after quantum transformer block.\n        \"\"\"\n        # Layer normalization followed by Quantum Attention\n        normalized_input = self.layer_norm_1(input_embeddings)\n        attention_output = self.quantum_attention(normalized_input)\n        embeddings_after_attention = input_embeddings + attention_output\n\n        # Layer normalization followed by classical MLP\n        embeddings_after_norm = self.layer_norm_2(embeddings_after_attention)\n        mlp_output = self.mlp_feedforward(embeddings_after_norm)\n        output_embeddings = embeddings_after_attention + mlp_output\n        return output_embeddings\n\n# Quantum Vision Transformer Model\nclass QuantumVisionTransformerModel(nn.Module):\n    def __init__(self, image_resolution, patch_resolution, input_channels, embedding_dimension, num_attention_heads, transformer_depth, mlp_hidden_dimension, num_output_classes, num_quantum_qubits=4):\n        \"\"\"\n        Quantum Vision Transformer (QViT) model with quantum-enhanced attention.\n\n        Args:\n            image_resolution (int): Height/Width of the input image.\n            patch_resolution (int): Height/Width of each patch.\n            input_channels (int): Number of input channels.\n            embedding_dimension (int): Embedding dimension for patches.\n            num_attention_heads (int): Number of attention heads in Transformer.\n            transformer_depth (int): Number of Transformer encoder blocks.\n            mlp_hidden_dimension (int): Hidden dimension in MLP layers.\n            num_output_classes (int): Number of output classes.\n            num_quantum_qubits (int): Number of qubits for quantum attention.\n        \"\"\"\n        super().__init__()\n        # Classical Patch Embedding remains\n        self.patch_embed = PatchEmbeddingLayer(image_resolution, patch_resolution, input_channels, embedding_dimension)\n        self.num_image_patches = self.patch_embed.num_image_patches\n\n        # Class token and positional embeddings (classical)\n        self.class_token = nn.Parameter(torch.randn(1, 1, embedding_dimension))\n        self.position_embeddings = nn.Parameter(torch.randn(1, self.num_image_patches + 1, embedding_dimension))\n\n        # Use Quantum Transformer Encoder Blocks\n        self.transformer_encoder_layers = nn.ModuleList([\n            QuantumTransformerEncoderBlock(embedding_dimension, num_attention_heads, mlp_hidden_dimension, num_quantum_qubits)\n            for _ in range(transformer_depth)\n        ])\n\n        self.layer_norm_final = nn.LayerNorm(embedding_dimension)\n        self.classification_head = nn.Linear(embedding_dimension, num_output_classes) # Classical classification head\n\n    def forward(self, input_images):\n        \"\"\"\n        Forward pass of the Quantum Vision Transformer Model.\n\n        Args:\n            input_images (torch.Tensor): Input images of shape (B, C, H, W).\n\n        Returns:\n            torch.Tensor: Classification output logits from QViT.\n        \"\"\"\n        # Input shape: (Batch Size, Channels, Height, Width)\n        batch_size_val = input_images.shape[0]\n\n        # Classical Patch Embedding\n        patches_embedded = self.patch_embed(input_images)  # Shape: (B, num_patches, embedding_dimension)\n\n        # Add Class token\n        class_tokens = self.class_token.expand(batch_size_val, -1, -1)  # Shape: (B, 1, embedding_dimension)\n        embeddings_with_class_token = torch.cat([class_tokens, patches_embedded], dim=1)  # Shape: (B, num_patches + 1, embedding_dimension)\n\n        # Add Positional Embeddings\n        transformer_input = embeddings_with_class_token + self.position_embeddings\n\n        # Pass through Quantum Transformer Blocks\n        transformer_output = transformer_input\n        for transformer_block in self.transformer_encoder_layers:\n            transformer_output = transformer_block(transformer_output)\n\n        # Normalize and extract class token for classification\n        class_token_embedding = self.layer_norm_final(transformer_output[:, 0])  # Shape: (B, embedding_dimension)\n\n        # Classical Classification Head\n        output_logits = self.classification_head(class_token_embedding)  # Shape: (B, num_output_classes)\n        return output_logits\n\n# Initialize the Quantum Vision Transformer Model\nquantum_vision_transformer_model = QuantumVisionTransformerModel(\n    image_resolution=image_resolution,\n    patch_resolution=patch_resolution,\n    input_channels=input_channels,\n    embedding_dimension=embedding_dimension,\n    num_attention_heads=num_attention_heads,\n    transformer_depth=transformer_depth,\n    mlp_hidden_dimension=mlp_hidden_dimension,\n    num_output_classes=num_output_classes,\n    num_quantum_qubits=num_quantum_qubits\n).to(computing_device)\n\nprint(f\"Quantum Vision Transformer initialized with {sum(p.numel() for p in quantum_vision_transformer_model.parameters())} parameters\")\n\n# Display Quantum Vision Transformer Architecture Summary\nprint(\"\\nQuantum Vision Transformer Architecture:\")\nprint(f\"- Image size: {image_resolution}x{image_resolution}\")\nprint(f\"- Patch size: {patch_resolution}x{patch_resolution}\")\nprint(f\"- Number of patches: {num_image_patches}\")\nprint(f\"- Embedding dimension: {embedding_dimension}\")\nprint(f\"- Number of attention heads: {num_attention_heads}\")\nprint(f\"- Number of transformer blocks: {transformer_depth}\")\nprint(f\"- MLP dimension: {mlp_hidden_dimension}\")\nprint(f\"- Number of qubits per patch: {num_quantum_qubits}\")\n\nprint(\"\\nKey Components of the Quantum Vision Transformer:\")\nprint(\"1. Classical Patch Embedding Layer: Divides the image into patches and embeds them using convolution.\")\nprint(\"2. Quantum Attention Mechanism: Enhances the query representation using quantum circuits for attention calculation.\")\nprint(\"3. Quantum Transformer Blocks: Combines quantum-enhanced attention with classical MLP layers in each transformer block.\")\nprint(\"4. Classical Classification Head: Uses a linear layer to classify based on the final class token embedding.\")\n\nprint(\"\\nDetails of Quantum Enhancement:\")\nprint(\"- Quantum Circuit: Employs AngleEmbedding for classical data encoding into quantum states.\")\nprint(\"- Entangling Layers: Utilizes StronglyEntanglingLayers to introduce entanglement and quantum non-linearity.\")\nprint(\"- Quantum Attention: Applies quantum processing specifically to the query vectors within the attention mechanism.\")\nprint(\"- Measurement: Uses PauliZ expectation values to extract classical information from quantum states for subsequent layers.\")\n\nprint(\"\\nPotential Extensions for Future Research:\")\nprint(\"1. Fully Quantum Patch Embedding: Replace classical convolution in patch embedding with quantum circuits.\")\nprint(\"2. Quantum MLP: Implement the feed-forward MLP layers using variational quantum circuits to explore further quantum benefits.\")\nprint(\"3. Quantum Position Encoding: Investigate quantum methods for encoding positional information of patches.\")\nprint(\"4. Quantum-Enhanced Key and Value: Extend quantum processing to key and value vectors in the attention mechanism, not just queries.\")\nprint(\"5. Hardware-Efficient Quantum Circuits: Optimize the quantum circuits for better performance on Noisy Intermediate-Scale Quantum (NISQ) devices.\")\nprint(\"6. Hybrid Classical-Quantum Training Strategies: Explore techniques like classical pre-training followed by quantum fine-tuning to stabilize and improve quantum training.\")\nprint(\"7. Quantum Advantage Benchmarking: Conduct rigorous comparative analysis against classical models to quantify and demonstrate potential quantum advantage.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T12:04:42.912816Z","iopub.execute_input":"2025-03-12T12:04:42.913095Z","iopub.status.idle":"2025-03-12T12:05:02.206807Z","shell.execute_reply.started":"2025-03-12T12:04:42.913074Z","shell.execute_reply":"2025-03-12T12:05:02.206070Z"}},"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading PennyLane-0.40.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy<2.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.4.2)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\nCollecting tomlkit (from pennylane)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nCollecting appdirs (from pennylane)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\nCollecting pennylane-lightning>=0.40 (from pennylane)\n  Downloading PennyLane_Lightning-0.40.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.2)\nCollecting diastatic-malt (from pennylane)\n  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (2.4.1)\nCollecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.40->pennylane)\n  Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from diastatic-malt->pennylane) (1.6.3)\nRequirement already satisfied: gast in /usr/local/lib/python3.10/dist-packages (from diastatic-malt->pennylane) (0.6.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from diastatic-malt->pennylane) (2.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2025.1.31)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\nRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1->pennylane) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1->pennylane) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.1->pennylane) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.1->pennylane) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.1->pennylane) (2024.2.0)\nDownloading PennyLane-0.40.0-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.40.0-cp310-cp310-manylinux_2_28_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: appdirs, tomlkit, scipy-openblas32, autoray, diastatic-malt, rustworkx, pennylane-lightning, pennylane\nSuccessfully installed appdirs-1.4.4 autoray-0.7.0 diastatic-malt-2.15.2 pennylane-0.40.0 pennylane-lightning-0.40.0 rustworkx-0.16.0 scipy-openblas32-0.3.29.0.0 tomlkit-0.13.2\nUsing device: cuda\nVision Transformer initialized with 72074 parameters\nStarting training process...\nEpoch [1/3], Step [10/79], Loss: 2.3731\nEpoch [1/3], Step [20/79], Loss: 2.2582\nEpoch [1/3], Step [30/79], Loss: 2.0780\nEpoch [1/3], Step [40/79], Loss: 1.7129\nEpoch [1/3], Step [50/79], Loss: 1.2407\nEpoch [1/3], Step [60/79], Loss: 0.8257\nEpoch [1/3], Step [70/79], Loss: 0.6130\nEpoch [1/3], Loss: 1.5706, Time: 1.41s\nEpoch [2/3], Step [10/79], Loss: 0.3996\nEpoch [2/3], Step [20/79], Loss: 0.2413\nEpoch [2/3], Step [30/79], Loss: 0.1688\nEpoch [2/3], Step [40/79], Loss: 0.1068\nEpoch [2/3], Step [50/79], Loss: 0.0852\nEpoch [2/3], Step [60/79], Loss: 0.1639\nEpoch [2/3], Step [70/79], Loss: 0.0651\nEpoch [2/3], Loss: 0.1824, Time: 0.51s\nEpoch [3/3], Step [10/79], Loss: 0.0402\nEpoch [3/3], Step [20/79], Loss: 0.0260\nEpoch [3/3], Step [30/79], Loss: 0.0385\nEpoch [3/3], Step [40/79], Loss: 0.0239\nEpoch [3/3], Step [50/79], Loss: 0.0149\nEpoch [3/3], Step [60/79], Loss: 0.0171\nEpoch [3/3], Step [70/79], Loss: 0.0146\nEpoch [3/3], Loss: 0.0243, Time: 0.50s\nTest Accuracy: 100.00%\nQuantum device initialized with 4 qubits\nQuantum Vision Transformer initialized with 72122 parameters\n\nQuantum Vision Transformer Architecture:\n- Image size: 28x28\n- Patch size: 7x7\n- Number of patches: 16\n- Embedding dimension: 64\n- Number of attention heads: 4\n- Number of transformer blocks: 2\n- MLP dimension: 128\n- Number of qubits per patch: 4\n\nKey Components of the Quantum Vision Transformer:\n1. Classical Patch Embedding Layer: Divides the image into patches and embeds them using convolution.\n2. Quantum Attention Mechanism: Enhances the query representation using quantum circuits for attention calculation.\n3. Quantum Transformer Blocks: Combines quantum-enhanced attention with classical MLP layers in each transformer block.\n4. Classical Classification Head: Uses a linear layer to classify based on the final class token embedding.\n\nDetails of Quantum Enhancement:\n- Quantum Circuit: Employs AngleEmbedding for classical data encoding into quantum states.\n- Entangling Layers: Utilizes StronglyEntanglingLayers to introduce entanglement and quantum non-linearity.\n- Quantum Attention: Applies quantum processing specifically to the query vectors within the attention mechanism.\n- Measurement: Uses PauliZ expectation values to extract classical information from quantum states for subsequent layers.\n\nPotential Extensions for Future Research:\n1. Fully Quantum Patch Embedding: Replace classical convolution in patch embedding with quantum circuits.\n2. Quantum MLP: Implement the feed-forward MLP layers using variational quantum circuits to explore further quantum benefits.\n3. Quantum Position Encoding: Investigate quantum methods for encoding positional information of patches.\n4. Quantum-Enhanced Key and Value: Extend quantum processing to key and value vectors in the attention mechanism, not just queries.\n5. Hardware-Efficient Quantum Circuits: Optimize the quantum circuits for better performance on Noisy Intermediate-Scale Quantum (NISQ) devices.\n6. Hybrid Classical-Quantum Training Strategies: Explore techniques like classical pre-training followed by quantum fine-tuning to stabilize and improve quantum training.\n7. Quantum Advantage Benchmarking: Conduct rigorous comparative analysis against classical models to quantify and demonstrate potential quantum advantage.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}